{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab2c08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available: using CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aadit\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#IMPORTS\n",
    "\n",
    "device = \"CPU\" # CPU or GPU (must set manually, don't know how to do this automatically in rapids 22.10)\n",
    "if device == \"GPU\":\n",
    "    print(\"CUDA is available: using GPU\")\n",
    "    import cudf as pd\n",
    "    import cupy as np\n",
    "    import cuml\n",
    "    from cuml.svm import SVR\n",
    "    from cuml.model_selection import train_test_split\n",
    "    from cuml.preprocessing import StandardScaler\n",
    "    from cuml.metrics import mean_squared_error as mse\n",
    "    def mape(y_true, y_pred):\n",
    "        return np.mean(np.abs(y_true-y_pred)/y_true)\n",
    "    import pynvml\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "else:\n",
    "    print(\"CUDA not available: using CPU\")\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.metrics import mean_squared_error as mse, mean_absolute_percentage_error as mape\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "from tqdm.auto import tqdm, trange\n",
    "import itertools\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import os\n",
    "import csv\n",
    "from sklearn.multioutput import RegressorChain\n",
    "import gc\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2c6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH\n",
    "\n",
    "noise = 30 # ADJUST level of gaussian noise added to outputs\n",
    "mod_type = 'svr'\n",
    "description = mod_type + '_noise-' + str(noise)\n",
    "result_dir = '../results/' + description + '/' # RESULTS stored in this folder\n",
    "# Check to see if directory exists, make it if it does not\n",
    "if(not os.path.exists(result_dir)):\n",
    "    os.makedirs(result_dir)\n",
    "data_path = '../dataset/fuchs_v3-2_seed-5_points_25000_noise_' + str(noise) + '.csv'  # CHANGE TO DESIRED DATA FILE\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d99fd44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "\n",
    "input_list = ['Intensity_(W_cm2)', 'Target_Thickness (um)', 'Focal_Distance_(um)'] # independent variables\n",
    "output_list = ['Max_Proton_Energy_(MeV)', 'Total_Proton_Energy_(MeV)', 'Avg_Proton_Energy_(MeV)',\n",
    "               'Max_Proton_Energy_Exact_(MeV)', 'Total_Proton_Energy_Exact_(MeV)', 'Avg_Proton_Energy_Exact_(MeV)'] # training outputs\n",
    "X = df[input_list].copy()\n",
    "y = df[output_list].copy()\n",
    "X[X.columns[0]] = np.log(X[X.columns[0]]) # Apply log scaling to intensity\n",
    "for col in y.columns:\n",
    "    y[col] = np.log(y[col]) # Apply log scaling to energy\n",
    "\n",
    "#Choose a datatype for X and y to be in\n",
    "dataType = 'float64'\n",
    "\n",
    "if device == \"GPU\":\n",
    "    X = X.to_cupy().astype(dtype=dataType)\n",
    "    y = y.to_cupy().astype(dtype=dataType)\n",
    "else:\n",
    "    X = X.to_numpy().astype(dtype=dataType)\n",
    "    y = y.to_numpy().astype(dtype=dataType)\n",
    "\n",
    "train_split = 0.8 # Reserve 80% of entire dataset for training\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X, y, train_size=train_split, shuffle = False)\n",
    "y_train_full = y_train_full[:, 0:3]\n",
    "y_test_full = y_test_full[:, 0:6] #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39f97bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training points:  [  500  1000  1500  2000  2500  3000  3500  4000  4500  5000  5500  6000\n",
      "  6500  7000  7500  8000  8500  9000  9500 10000 10500 11000 11500 12000\n",
      " 12500 13000 13500 14000 14500 15000 15500 16000 16500 17000 17500 18000\n",
      " 18500 19000 19500 20000]\n"
     ]
    }
   ],
   "source": [
    "#Hyperparams\n",
    "\n",
    "C = 2.5\n",
    "epsilon = 1e-2\n",
    "tol = 1e-3\n",
    "\n",
    "num_inputs = X_train_full.shape[1]\n",
    "num_outputs = y_train_full.shape[1]\n",
    "\n",
    "num_splits = 40\n",
    "data_fractions =  np.linspace(1/num_splits, 1, num_splits) # 1k, 2k, ..., 20k training points\n",
    "points_train = np.round(data_fractions*len(X_train_full)).astype(int)\n",
    "points_test = np.round(data_fractions*len(X_test_full)).astype(int)\n",
    "print('training points: ', points_train)\n",
    "\n",
    "time_list = np.zeros(num_splits)\n",
    "train_mse = np.zeros((num_splits, num_outputs))\n",
    "train_mape = np.zeros((num_splits, num_outputs))\n",
    "test_mse_noisy = np.zeros((num_splits, num_outputs))\n",
    "test_mape_noisy = np.zeros((num_splits, num_outputs))\n",
    "test_mse_exact = np.zeros((num_splits, num_outputs))\n",
    "test_mape_exact = np.zeros((num_splits, num_outputs))\n",
    "test_mape_exact_uncorrected = np.zeros((num_splits, num_outputs))\n",
    "mem_used = np.zeros((num_splits, num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a30b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theoretical correction factor:  1.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training split:  85%|████████▌ | 34/40 [35:46<14:29, 144.84s/it, test mape=2.59, corr fac=1.045, E=tot, mem=0]"
     ]
    }
   ],
   "source": [
    "# Train Loop\n",
    "\n",
    "print('theoretical correction factor: ', round(np.sqrt(1 + (noise/100)**2).tolist(), 3))\n",
    "outs = ['max', 'tot', 'avg']\n",
    "pbar = trange(num_splits, desc='training split', leave=True)\n",
    "for i in pbar:\n",
    "\n",
    "    X_train = X_train_full[0:points_train[i], 0:num_inputs].copy()\n",
    "    y_train = y_train_full[0:points_train[i]].copy()\n",
    "\n",
    "    X_test = X_test_full[0:points_test[i], 0:num_inputs].copy()\n",
    "    y_test_noisy = y_test_full[0:points_test[i]].copy()\n",
    "    y_test_exact = y_test_full[0:points_test[i], num_inputs:num_inputs+num_outputs].copy()\n",
    "\n",
    "    # StandardScaler: z-score normalization\n",
    "    ss_in = StandardScaler()\n",
    "    ss_in.fit(X_train)\n",
    "    X_train_norm = ss_in.transform(X_train)\n",
    "    X_test_norm = ss_in.transform(X_test)\n",
    "\n",
    "    ss_out = StandardScaler()\n",
    "    ss_out.fit(y_train)\n",
    "    y_train_norm = ss_out.transform(y_train)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    svrs = []\n",
    "    for j in range(num_outputs):\n",
    "        # Train SVR\n",
    "        svrs.append(SVR(C=C, epsilon=epsilon, tol=tol))\n",
    "        svrs[j].fit(X_train_norm, y_train_norm[:, j])\n",
    "\n",
    "        # Make Predictions\n",
    "        y_train_predict = svrs[j].predict(X_train_norm)\n",
    "        y_test_predict = svrs[j].predict(X_test_norm)\n",
    "        y_train_predict_unscaled = np.exp(ss_out.inverse_transform(y_train_predict.reshape(-1, 1).repeat(3, 1)))[:, j]\n",
    "        y_test_predict_unscaled = np.exp(ss_out.inverse_transform(y_test_predict.reshape(-1, 1).repeat(3, 1)))[:, j]\n",
    "\n",
    "        # Correct for Log Scaling\n",
    "        correction_factor = np.mean(np.exp(y_train[:, j])/y_train_predict_unscaled)\n",
    "        y_train_predict_corrected = y_train_predict_unscaled*correction_factor\n",
    "        y_test_predict_corrected = y_test_predict_unscaled*correction_factor\n",
    "\n",
    "        # Record Metrics\n",
    "        train_mse[i, j] = mse(np.exp(y_train[:, j]), y_train_predict_corrected)\n",
    "        train_mape[i, j] = mape(np.exp(y_train[:, j]), y_train_predict_corrected)\n",
    "        test_mse_noisy[i, j] = mse(np.exp(y_test_noisy[:, j]), y_test_predict_corrected)\n",
    "        test_mape_noisy[i, j] = mape(np.exp(y_test_noisy[:, j]), y_test_predict_corrected)\n",
    "        test_mse_exact[i, j] = mse(np.exp(y_test_exact[:, j]), y_test_predict_corrected)\n",
    "        test_mape_exact[i, j] = mape(np.exp(y_test_exact[:, j]), y_test_predict_corrected)\n",
    "        test_mape_exact_uncorrected[i, j] = mape(np.exp(y_test_exact[:, j]), y_test_predict_unscaled)\n",
    "\n",
    "        # Append output to input for next iteration in chained output regression\n",
    "        X_train_norm = np.concatenate([X_train_norm, y_train_predict.reshape(-1, 1)], axis=1)\n",
    "        X_test_norm = np.concatenate([X_test_norm, y_test_predict.reshape(-1, 1)], axis=1)\n",
    "\n",
    "        # Record memory usage\n",
    "        if device == \"GPU\":\n",
    "            mem = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            mem_used[i, j] = mem.used/2**30\n",
    "\n",
    "        pbar.set_postfix({'test mape':str(round(test_mape_exact[i, j].tolist()*100, 2)), 'corr fac': str(round(correction_factor.tolist(), 3)), 'E': outs[j], 'mem': round(mem_used[i,j].tolist(), 2)})\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_list[i] = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b0936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Memory consumption\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(points_train.get(), np.mean(mem_used, axis=1).get(), marker='s')\n",
    "ax.set_ylim([0, 32])\n",
    "ax.set_xlabel(\"Number of Points\")\n",
    "ax.set_ylabel(\"Average GPU Memory (GiB)\")\n",
    "#fig.savefig(result_dir+'gpu_mem.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a1961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time To Train\n",
    "\n",
    "print('total run time: ', sum(time_list), ' s')\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_title(\"Time to Train {}\".format(mod_type.upper()))\n",
    "ax.set_xlabel(\"Number of Points\")\n",
    "ax.set_ylabel(\"Time in Seconds\")\n",
    "ax.plot(points_train.get(), time_list.get(), marker='s', c='c', label='Time')\n",
    "#fig.savefig(result_dir+'time.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d08b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "for j in range(num_outputs):\n",
    "    ax[j].plot(points_train.get(), train_mse[:, j].get(), c='b', label='train')\n",
    "    ax[j].plot(points_train.get(), test_mse_noisy[:, j].get(), c='g', label='test noisy', linestyle='dashed')\n",
    "    ax[j].plot(points_train.get(), test_mse_exact[:, j].get(), c='r', label='test exact', linestyle='dotted')\n",
    "    ax[j].set_xlabel(\"Number of Points\")\n",
    "ax[0].set_ylabel(\"Mean Squared Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b3e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE\n",
    "\n",
    "print('total run time: ', sum(time_list), ' s')\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 5))\n",
    "for j in range(num_outputs):\n",
    "    print(num_outputs)\n",
    "    ax[j].plot(points_train.get(), train_mape[:, j].get()*100, c='b', label='train')\n",
    "    ax[j].plot(points_train.get(), test_mape_noisy[:, j].get()*100, c='g', label='test noisy', linestyle='dashed')\n",
    "    ax[j].plot(points_train.get(), test_mape_exact[:, j].get()*100, c='r', label='test exact', linestyle='dotted')\n",
    "    ax[j].plot(points_train.get(), test_mape_exact_uncorrected[:, j].get()*100, c='k', label='test exact uncorrected', linestyle='dotted')\n",
    "    ax[j].set_xlabel(\"Number of Points\")\n",
    "    ax[j].set_ylim(0, noise+5)\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].set_ylabel(\"Mean Absolute Percentage Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE CSV\n",
    "\n",
    "columns = ['train_points', 'gpu_memory', 'time',\n",
    "            'train_mse_max', 'train_mse_tot', 'train_mse_avg',\n",
    "            'test_mse_noisy_max', 'test_mse_noisy_tot', 'test_mse_noisy_avg',\n",
    "            'test_mse_exact_max', 'test_mse_exact_tot', 'test_mse_exact_avg',\n",
    "            'train_mape_max', 'train_mape_tot', 'train_mape_avg',\n",
    "            'test_mape_noisy_max', 'test_mape_noisy_tot', 'test_mape_noisy_avg',\n",
    "            'test_mape_exact_max', 'test_mape_exact_tot', 'test_mape_exact_avg',\n",
    "            'test_mape_exact_uncorrected_max', 'test_mape_exact_uncorrected_tot', 'test_mape_exact_uncorrected_avg']\n",
    "data = [points_train, np.mean(mem_used, axis=1), time_list,\n",
    "        train_mse[:, 0], train_mse[:, 1], train_mse[:, 2],\n",
    "        test_mse_noisy[:, 0], test_mse_noisy[:, 1], test_mse_noisy[:, 2],\n",
    "        test_mse_exact[:, 0], test_mse_exact[:, 1], test_mse_exact[:, 2],\n",
    "        train_mape[:, 0], train_mape[:, 1], train_mape[:, 2],\n",
    "        test_mape_noisy[:, 0], test_mape_noisy[:, 1], test_mape_noisy[:, 2],\n",
    "        test_mape_exact[:, 0], test_mape_exact[:, 1], test_mape_exact[:, 2],\n",
    "        test_mape_exact_uncorrected[:, 0], test_mape_exact_uncorrected[:, 1], test_mape_exact_uncorrected[:, 2]]\n",
    "output_df = pd.DataFrame(dict(zip(columns, data)))\n",
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626afc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_pandas().to_csv(result_dir + 'metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
